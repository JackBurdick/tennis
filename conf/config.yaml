defaults:
  - override hydra/sweeper: optuna

trainer:
  _target_: trainer.Trainer
  # episode
  n_episodes: 10000
  max_t: 2000
  
  # logistical
  run_headless: True
  print_setup: True
  print_every: 100
  target_score: 0.5
  window_size: 100
  scores_path: scores.pkl

  ma_cfg:
    n_agents: 2
    batch_size: 512
    buffer_size: 100_000  # replay buffer size

  agent_cfg:
    actor:
      lr: 0.001
      #hidden_units: [512, 256, 64]
      fc_1: 400
      fc_2: 300
      #fc_3: 64
    critic:
      lr: 0.001
      weight_decay: 0.0
      fc_1: 400
      fc_2: 300
      #fc_3: 64
      # dropout: 0.2
    learn_iterations: 10
    update_every: 20
    gamma: 0.99  # discount factor
    tau: 0.001  # soft update of target parameters

    oun:
      mu: 0.0
      theta: 0.15
      sigma: 0.2
    seed: 0

hydra:
  sweeper:
    sampler:
      seed: 42
    direction: maximize
    study_name: rl_1
    storage: "sqlite:///hydra_test_1.db"
    #load_if_exists: True
    n_trials: 40
    n_jobs: 1
    params:
      # actor
      trainer.agent_cfg.actor.fc_1: range(128, 512, step=64)
      trainer.agent_cfg.actor.fc_2: range(64, 384, step=64)
      trainer.agent_cfg.actor.lr: interval(0.0001, 0.001)

      # critic
      trainer.agent_cfg.critic.fc_1: range(128, 512, step=64)
      trainer.agent_cfg.critic.fc_2: range(64, 384, step=64)
      trainer.agent_cfg.critic.lr: interval(0.0001, 0.001)

      # training
      trainer.agent_cfg.update_every: range(10, 20, step=5)
      trainer.agent_cfg.learn_iterations: range(5, 15, step=5)
      

      trainer.n_episodes: 1500
      trainer.print_setup: False
      trainer.target_score: 5.0
